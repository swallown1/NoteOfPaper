## Heterogeneous Network Representation Learning

### 摘要
本片论文主要对异质图表示学习进行回顾，探讨其不用的类别以及联系。

异质图表示学习的主要目的：通过自动编码的方式将异质图网络中的结构以及关系自动编码即潜在嵌入空间。

学习到的嵌入表示可以用于更多其他算法来解决不同的任务。

现如今的嵌入表示主要分为2中类别：shallow embedding learning and graph neural networks


### 介绍

在很多领域中通常都是一个很复杂的系统，其中包括很多不同类别的实体以及实体之间不同类型联系。

通常一个异质图会定义成一个有向图  $G=(V,E)$  其中V是实体，E是边类型。$T_V$ 和 $T_E$分别是实体和边的映射集合，即实体和边的集合。其中满足$|T_V|+|T_E|>2$。
如果两条边的类别相同，对于源实体和目标实体会共享相同的边类型。如果$T_V=1$ 和 $T_E=1$ 那么这就变成了一个同质图。

异构网络的常规挖掘过程通常从提取类型化的结构特征开始，然后将其进一步输入到用于下游图形任务的机器学习模型中。
通常，这种挖掘和学习工作流涉及异构网络中元关系，网络模式和元路径的独特概念，其介绍如下![Mining Heterogeneous Information Networks: Principles and Methodologies](Mining Heterogeneous Information Networks: Principles and Methodologies).。

Meta Relation:对于每条边连接着源实体S以及目标实体T，边e的meta relation定义为<T(s),φ(e),T(t)>。例如作者和论文之间的边的meta relation为 作者 写 论文

Network Schema：有图G中所有的Meta Relation组成，$T_G = (T_V , T_E)$

Meta Path:一个元路径 P 定义为由Meta Relation组成的序列。
例如：一个元路径OAG表示：“author-paper-venue-paper-author”，这表示两个作者在同一个期刊发表论文的情况。

最近异构网络挖掘范例已转向基于表示学习的技术。其目标是自动学习潜在低维嵌入。无论网络中的实体，边或者子图。

本片论文指出了表示学习的挑战，然后将现有解决方案分为两类，即基于浅层低嵌入和图神经网络（GNN）的开发。


###  Heterogeneous Network Mining（异构网络挖掘）
已有研究探索包含不同类型的顶点和边的网络，
例如推荐系统中的二部图[Koren等人，2009年]，超图[Zhou等人，2007年]，多层网络[Kivela¨等人，2014年]和书目图形网络中的主题建模[Tang等人，2008年]。

异构网络的形式化可以追溯到对异构信息网络的研究[Sun et al。，2009]，作者提出利用不同类型顶点之间的链接来生成聚类。更重要的是，
作者认为“多类型对象之间的交互作用在揭示网络所承载的丰富语义方面起着关键作用” [Sun and Han，2012]。 [聚类思想]

除了聚类之外，其他图挖掘任务也证明了图异质性的重要性，包括顶点分类，排名[Ji等，2011]，相似度搜索[Sun等，2011]，链接预测[Dong等，2015]，异常检测[Chen等，2016]等

挖掘异构网络的常规工作流程是，首先在网络模式上定义元路径或其变体，例如元图或元结构，然后将其用作开发机器学习的功能模型[Sun and Han，2012]。
例如：以学术网络中的链接预测任务为例，我们可以定义“作者论文-作者”元路径以提取异构结构特征，并使用它们推断每对作者之间是否存在协作关系。


Challenges:最主要的挑战主要是元路径的设计。通常，为了解决输入网络的特定图形问题，我们必须手动自定义特定于任务和数据的元路径，这需要相关领域的知识和经验。

另外，元路径的常规用法限于离散空间。 如果图中没有在结构上连接两个顶点，则基于元路径的技术将无法捕获它们之间的关系。 

例如，假设一位学者在NeurIPS上发表论文，而另一位学者在ICML上发表论文。 根据“作者-论文-地点-论文-作者”的元路径，它们之间的相似度为零，
这是由于NeurIPS和ICML之间的强烈关联而违反直觉的[Dong等，2017]。 通过使用连续空间中的潜在表示，可以自然解决由离散结构引起的挑战。

### Heterogeneous Network Representation
在过去的几年中，旨在通过使用神经网络来学习对象的潜在嵌入的神经表示学习为各种领域提供了革命性的成果，
受此启发，人们尝试将表示学习应用于网络。网络表示学习的前提是，它无需手动构造结构特征，而是可以自动将网络结构嵌入到潜在空间中，
然后将其用于现有的网络挖掘任务。

在网络上进行神经表示学习的主要问题是非欧图结构向欧氏嵌入空间的转换，因为图数据和神经网络之间存在间隙。

最近，弥合这一差距的主要进展是借鉴了图论的思想。例如，已经做出了一个尝试，以利用随机游走将图结构转换为序列，可以被基于序列的嵌入学习算法
（例如DeepWalk和node2vec模型）所消耗[Perozzi等人，2014； Grover和Leskovec，2016年]。
在图谱和卷积滤波器之间的连接上可以找到另一条努力路线，通过它们我们可以直接在图上设计神经网络操作，
例如图卷积网络（GCN）[Kipf and Welling，2017]。

异构网络上的大多数表示学习技术都是受（均匀）网络表示学习启发并基于这种技术开发的。**核心问题是如何将不同类型的顶点和边缘之间的结构转换为潜在空间**，
以便可以对异构网络的结构和语义属性进行编码和保留。

#### Heterogeneous Network Embedding
我们回顾了异构网络基于浅层学习的嵌入方法。 “浅”方法的特征在于嵌入查找表，这意味着它们将每个顶点直接编码为矢量，并且该嵌入表是要优化的参数。 
沿着这条思路，可以基于它们所基于的假设对方法进行分类。

**1、Distributional Hypothesis based Methods**
