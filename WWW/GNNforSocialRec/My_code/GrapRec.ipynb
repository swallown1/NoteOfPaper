{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "import datetime\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    parser = argparse.ArgumentParser(description='Social Recommendation: GraphRec model')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, metavar='N', help='input batch size for training')\n",
    "    parser.add_argument('--embed_dim', type=int, default=64, metavar='N', help='embedding size')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR', help='learning rate')\n",
    "    parser.add_argument('--test_batch_size', type=int, default=1000, metavar='N', help='input batch size for testing')\n",
    "    parser.add_argument('--epochs', type=int, default=100, metavar='N', help='number of epochs to train')\n",
    "    args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device( \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = args.embed_dim\n",
    "dir_data = './data/toy_dataset'\n",
    "path_data = dir_data + \".pickle\"\n",
    "data_file = open(path_data, 'rb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "history_u_lists, \n",
    "\n",
    "history_ur_lists, \n",
    "\n",
    "history_v_lists, \n",
    "\n",
    "history_vr_lists, \n",
    "\n",
    "train_u,  14091\n",
    "\n",
    "train_v, \n",
    "\n",
    "train_r, \n",
    "\n",
    "test_u, \n",
    "\n",
    "test_v, \n",
    "\n",
    "test_r, \n",
    "\n",
    "social_adj_lists, set集合，{0:{1,2,5,8},1:{0,5,8}}\n",
    "\n",
    "ratings_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_u_lists, history_ur_lists, history_v_lists, history_vr_lists, train_u, train_v, train_r, test_u, test_v, test_r, social_adj_lists, ratings_list = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([681,  81, 172,  ..., 518, 377, 479])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor(train_u)\n",
    "# len(train_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# history_u_lists, history_ur_lists:\n",
    "# 用户的购买历史记录（在培训集中设置的项目）以及他/她的评分得分（dict）\n",
    "# history_v_lists, history_vr_lists\n",
    "# 与项目互动的用户集（在培训集中）和评分分数（dict）\n",
    "\n",
    "# train_u, train_v, train_r: 训练集 (user, item, rating)\n",
    "# test_u, test_v, test_r: 测试集 (user, item, rating)\n",
    "\n",
    "# social_adj_lists: user相邻的邻居\n",
    "# ratings_list: 评分值从0.5到4.0（8个维度嵌入）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三个元素的元祖的集合  是用户u对项目v的打分r   14091*()\n",
    "trainset = torch.utils.data.TensorDataset(torch.LongTensor(train_u), torch.LongTensor(train_v),\n",
    "                                              torch.FloatTensor(train_r))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14091"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(681), tensor(0), tensor(2.))\n",
      "(tensor(81), tensor(1), tensor(1.))\n",
      "(tensor(172), tensor(2), tensor(3.))\n",
      "(tensor(83), tensor(3), tensor(3.))\n",
      "(tensor(151), tensor(4), tensor(3.))\n",
      "(tensor(27), tensor(5), tensor(4.))\n",
      "(tensor(197), tensor(6), tensor(2.5000))\n",
      "(tensor(316), tensor(7), tensor(3.5000))\n",
      "(tensor(385), tensor(8), tensor(2.))\n",
      "(tensor(231), tensor(9), tensor(3.5000))\n",
      "(tensor(646), tensor(10), tensor(2.))\n",
      "(tensor(278), tensor(11), tensor(4.))\n",
      "(tensor(23), tensor(12), tensor(3.))\n",
      "(tensor(477), tensor(13), tensor(3.5000))\n",
      "(tensor(392), tensor(14), tensor(3.5000))\n",
      "(tensor(145), tensor(15), tensor(3.5000))\n",
      "(tensor(35), tensor(16), tensor(2.5000))\n",
      "(tensor(292), tensor(17), tensor(4.))\n",
      "(tensor(74), tensor(18), tensor(1.5000))\n",
      "(tensor(669), tensor(19), tensor(4.))\n",
      "(tensor(528), tensor(20), tensor(3.5000))\n",
      "(tensor(522), tensor(21), tensor(3.))\n",
      "(tensor(590), tensor(22), tensor(1.5000))\n",
      "(tensor(552), tensor(23), tensor(1.5000))\n",
      "(tensor(336), tensor(20), tensor(3.5000))\n",
      "(tensor(676), tensor(24), tensor(3.5000))\n",
      "(tensor(225), tensor(25), tensor(3.))\n",
      "(tensor(674), tensor(26), tensor(2.5000))\n",
      "(tensor(172), tensor(27), tensor(3.5000))\n",
      "(tensor(170), tensor(13), tensor(4.))\n",
      "(tensor(88), tensor(28), tensor(3.))\n",
      "(tensor(24), tensor(10), tensor(3.))\n",
      "(tensor(262), tensor(29), tensor(3.5000))\n",
      "(tensor(210), tensor(30), tensor(3.))\n",
      "(tensor(594), tensor(31), tensor(3.5000))\n",
      "(tensor(119), tensor(32), tensor(4.))\n",
      "(tensor(504), tensor(33), tensor(4.))\n",
      "(tensor(139), tensor(34), tensor(3.))\n",
      "(tensor(13), tensor(35), tensor(1.5000))\n",
      "(tensor(76), tensor(36), tensor(3.5000))\n",
      "(tensor(612), tensor(37), tensor(4.))\n",
      "(tensor(146), tensor(38), tensor(4.))\n",
      "(tensor(396), tensor(6), tensor(2.5000))\n",
      "(tensor(489), tensor(10), tensor(4.))\n",
      "(tensor(277), tensor(39), tensor(3.))\n",
      "(tensor(9), tensor(40), tensor(3.5000))\n",
      "(tensor(258), tensor(34), tensor(3.))\n",
      "(tensor(1), tensor(34), tensor(4.))\n",
      "(tensor(81), tensor(41), tensor(0.5000))\n",
      "(tensor(520), tensor(6), tensor(1.5000))\n",
      "(tensor(401), tensor(42), tensor(3.))\n",
      "(tensor(341), tensor(43), tensor(3.))\n",
      "(tensor(41), tensor(44), tensor(0.5000))\n",
      "(tensor(398), tensor(45), tensor(4.))\n",
      "(tensor(133), tensor(6), tensor(3.))\n",
      "(tensor(693), tensor(46), tensor(4.))\n",
      "(tensor(427), tensor(47), tensor(1.))\n",
      "(tensor(219), tensor(45), tensor(3.5000))\n",
      "(tensor(401), tensor(48), tensor(4.))\n",
      "(tensor(184), tensor(49), tensor(2.))\n",
      "(tensor(459), tensor(11), tensor(4.))\n",
      "(tensor(43), tensor(50), tensor(1.5000))\n",
      "(tensor(601), tensor(51), tensor(3.))\n",
      "(tensor(70), tensor(52), tensor(3.5000))\n",
      "(tensor(626), tensor(53), tensor(4.))\n",
      "(tensor(637), tensor(54), tensor(4.))\n",
      "(tensor(115), tensor(6), tensor(3.))\n",
      "(tensor(358), tensor(24), tensor(3.5000))\n",
      "(tensor(82), tensor(39), tensor(2.))\n",
      "(tensor(521), tensor(33), tensor(4.))\n",
      "(tensor(377), tensor(55), tensor(2.5000))\n",
      "(tensor(529), tensor(42), tensor(2.5000))\n",
      "(tensor(24), tensor(56), tensor(4.))\n",
      "(tensor(509), tensor(57), tensor(2.5000))\n",
      "(tensor(367), tensor(47), tensor(3.5000))\n",
      "(tensor(420), tensor(39), tensor(3.5000))\n",
      "(tensor(34), tensor(11), tensor(2.5000))\n",
      "(tensor(458), tensor(58), tensor(3.5000))\n",
      "(tensor(193), tensor(59), tensor(1.5000))\n",
      "(tensor(117), tensor(32), tensor(3.))\n",
      "(tensor(593), tensor(6), tensor(4.))\n",
      "(tensor(683), tensor(60), tensor(4.))\n",
      "(tensor(298), tensor(61), tensor(2.5000))\n",
      "(tensor(445), tensor(62), tensor(3.5000))\n",
      "(tensor(198), tensor(42), tensor(3.5000))\n",
      "(tensor(692), tensor(51), tensor(2.5000))\n",
      "(tensor(172), tensor(63), tensor(3.))\n",
      "(tensor(241), tensor(64), tensor(3.))\n",
      "(tensor(408), tensor(33), tensor(3.5000))\n",
      "(tensor(703), tensor(65), tensor(0.5000))\n",
      "(tensor(315), tensor(66), tensor(3.5000))\n",
      "(tensor(519), tensor(62), tensor(4.))\n",
      "(tensor(132), tensor(67), tensor(2.5000))\n",
      "(tensor(447), tensor(68), tensor(4.))\n",
      "(tensor(227), tensor(69), tensor(3.))\n",
      "(tensor(74), tensor(70), tensor(1.5000))\n",
      "(tensor(301), tensor(45), tensor(4.))\n",
      "(tensor(482), tensor(71), tensor(3.5000))\n",
      "(tensor(463), tensor(72), tensor(1.))\n",
      "(tensor(217), tensor(73), tensor(2.))\n",
      "(tensor(53), tensor(45), tensor(1.))\n",
      "(tensor(462), tensor(46), tensor(4.))\n",
      "(tensor(297), tensor(74), tensor(3.5000))\n",
      "(tensor(38), tensor(75), tensor(2.5000))\n",
      "(tensor(42), tensor(76), tensor(3.))\n",
      "(tensor(253), tensor(77), tensor(4.))\n",
      "(tensor(256), tensor(67), tensor(2.))\n",
      "(tensor(466), tensor(8), tensor(1.5000))\n",
      "(tensor(75), tensor(78), tensor(3.5000))\n",
      "(tensor(696), tensor(60), tensor(4.))\n",
      "(tensor(139), tensor(79), tensor(3.5000))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    print(trainset.__getitem__(i))\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(681), tensor(0), tensor(2.))\n",
      "(tensor(81), tensor(1), tensor(1.))\n",
      "(tensor(172), tensor(2), tensor(3.))\n",
      "(tensor(83), tensor(3), tensor(3.))\n",
      "(tensor(151), tensor(4), tensor(3.))\n",
      "(tensor(27), tensor(5), tensor(4.))\n",
      "(tensor(197), tensor(6), tensor(2.5000))\n",
      "(tensor(316), tensor(7), tensor(3.5000))\n",
      "(tensor(385), tensor(8), tensor(2.))\n",
      "(tensor(231), tensor(9), tensor(3.5000))\n",
      "(tensor(646), tensor(10), tensor(2.))\n",
      "(tensor(278), tensor(11), tensor(4.))\n",
      "(tensor(23), tensor(12), tensor(3.))\n",
      "(tensor(477), tensor(13), tensor(3.5000))\n",
      "(tensor(392), tensor(14), tensor(3.5000))\n",
      "(tensor(145), tensor(15), tensor(3.5000))\n",
      "(tensor(35), tensor(16), tensor(2.5000))\n",
      "(tensor(292), tensor(17), tensor(4.))\n",
      "(tensor(74), tensor(18), tensor(1.5000))\n",
      "(tensor(669), tensor(19), tensor(4.))\n",
      "(tensor(528), tensor(20), tensor(3.5000))\n",
      "(tensor(522), tensor(21), tensor(3.))\n",
      "(tensor(590), tensor(22), tensor(1.5000))\n",
      "(tensor(552), tensor(23), tensor(1.5000))\n",
      "(tensor(336), tensor(20), tensor(3.5000))\n",
      "(tensor(676), tensor(24), tensor(3.5000))\n",
      "(tensor(225), tensor(25), tensor(3.))\n",
      "(tensor(674), tensor(26), tensor(2.5000))\n",
      "(tensor(172), tensor(27), tensor(3.5000))\n",
      "(tensor(170), tensor(13), tensor(4.))\n",
      "(tensor(88), tensor(28), tensor(3.))\n",
      "(tensor(24), tensor(10), tensor(3.))\n",
      "(tensor(262), tensor(29), tensor(3.5000))\n",
      "(tensor(210), tensor(30), tensor(3.))\n",
      "(tensor(594), tensor(31), tensor(3.5000))\n",
      "(tensor(119), tensor(32), tensor(4.))\n",
      "(tensor(504), tensor(33), tensor(4.))\n",
      "(tensor(139), tensor(34), tensor(3.))\n",
      "(tensor(13), tensor(35), tensor(1.5000))\n",
      "(tensor(76), tensor(36), tensor(3.5000))\n",
      "(tensor(612), tensor(37), tensor(4.))\n",
      "(tensor(146), tensor(38), tensor(4.))\n",
      "(tensor(396), tensor(6), tensor(2.5000))\n",
      "(tensor(489), tensor(10), tensor(4.))\n",
      "(tensor(277), tensor(39), tensor(3.))\n",
      "(tensor(9), tensor(40), tensor(3.5000))\n",
      "(tensor(258), tensor(34), tensor(3.))\n",
      "(tensor(1), tensor(34), tensor(4.))\n",
      "(tensor(81), tensor(41), tensor(0.5000))\n",
      "(tensor(520), tensor(6), tensor(1.5000))\n",
      "(tensor(401), tensor(42), tensor(3.))\n",
      "(tensor(341), tensor(43), tensor(3.))\n",
      "(tensor(41), tensor(44), tensor(0.5000))\n",
      "(tensor(398), tensor(45), tensor(4.))\n",
      "(tensor(133), tensor(6), tensor(3.))\n",
      "(tensor(693), tensor(46), tensor(4.))\n",
      "(tensor(427), tensor(47), tensor(1.))\n",
      "(tensor(219), tensor(45), tensor(3.5000))\n",
      "(tensor(401), tensor(48), tensor(4.))\n",
      "(tensor(184), tensor(49), tensor(2.))\n",
      "(tensor(459), tensor(11), tensor(4.))\n",
      "(tensor(43), tensor(50), tensor(1.5000))\n",
      "(tensor(601), tensor(51), tensor(3.))\n",
      "(tensor(70), tensor(52), tensor(3.5000))\n",
      "(tensor(626), tensor(53), tensor(4.))\n",
      "(tensor(637), tensor(54), tensor(4.))\n",
      "(tensor(115), tensor(6), tensor(3.))\n",
      "(tensor(358), tensor(24), tensor(3.5000))\n",
      "(tensor(82), tensor(39), tensor(2.))\n",
      "(tensor(521), tensor(33), tensor(4.))\n",
      "(tensor(377), tensor(55), tensor(2.5000))\n",
      "(tensor(529), tensor(42), tensor(2.5000))\n",
      "(tensor(24), tensor(56), tensor(4.))\n",
      "(tensor(509), tensor(57), tensor(2.5000))\n",
      "(tensor(367), tensor(47), tensor(3.5000))\n",
      "(tensor(420), tensor(39), tensor(3.5000))\n",
      "(tensor(34), tensor(11), tensor(2.5000))\n",
      "(tensor(458), tensor(58), tensor(3.5000))\n",
      "(tensor(193), tensor(59), tensor(1.5000))\n",
      "(tensor(117), tensor(32), tensor(3.))\n",
      "(tensor(593), tensor(6), tensor(4.))\n",
      "(tensor(683), tensor(60), tensor(4.))\n",
      "(tensor(298), tensor(61), tensor(2.5000))\n",
      "(tensor(445), tensor(62), tensor(3.5000))\n",
      "(tensor(198), tensor(42), tensor(3.5000))\n",
      "(tensor(692), tensor(51), tensor(2.5000))\n",
      "(tensor(172), tensor(63), tensor(3.))\n",
      "(tensor(241), tensor(64), tensor(3.))\n",
      "(tensor(408), tensor(33), tensor(3.5000))\n",
      "(tensor(703), tensor(65), tensor(0.5000))\n",
      "(tensor(315), tensor(66), tensor(3.5000))\n",
      "(tensor(519), tensor(62), tensor(4.))\n",
      "(tensor(132), tensor(67), tensor(2.5000))\n",
      "(tensor(447), tensor(68), tensor(4.))\n",
      "(tensor(227), tensor(69), tensor(3.))\n",
      "(tensor(74), tensor(70), tensor(1.5000))\n",
      "(tensor(301), tensor(45), tensor(4.))\n",
      "(tensor(482), tensor(71), tensor(3.5000))\n",
      "(tensor(463), tensor(72), tensor(1.))\n",
      "(tensor(217), tensor(73), tensor(2.))\n",
      "(tensor(53), tensor(45), tensor(1.))\n",
      "(tensor(462), tensor(46), tensor(4.))\n",
      "(tensor(297), tensor(74), tensor(3.5000))\n",
      "(tensor(38), tensor(75), tensor(2.5000))\n",
      "(tensor(42), tensor(76), tensor(3.))\n",
      "(tensor(253), tensor(77), tensor(4.))\n",
      "(tensor(256), tensor(67), tensor(2.))\n",
      "(tensor(466), tensor(8), tensor(1.5000))\n",
      "(tensor(75), tensor(78), tensor(3.5000))\n",
      "(tensor(696), tensor(60), tensor(4.))\n",
      "(tensor(139), tensor(79), tensor(3.5000))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    print(train_loader.dataset.__getitem__(i))\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = torch.utils.data.TensorDataset(torch.LongTensor(test_u), torch.LongTensor(test_v),\n",
    "                                             torch.FloatTensor(test_r))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=args.test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = history_u_lists.__len__() #705 users\n",
    "num_items = history_v_lists.__len__()  #1941 item\n",
    "num_ratings = ratings_list.__len__()  #8  8种分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2e = nn.Embedding(num_users, embed_dim).to(device)\n",
    "v2e = nn.Embedding(num_items, embed_dim).to(device)\n",
    "\n",
    "# 将评分也转化成Embedding\n",
    "r2e = nn.Embedding(num_ratings, embed_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0408,  1.2600, -1.9267,  ..., -1.2854, -1.6544, -0.3633],\n",
       "        [-2.5353, -2.4534, -0.2431,  ...,  0.5663, -0.1036, -1.1639],\n",
       "        [-0.6861, -0.5613,  0.6603,  ...,  1.0003, -0.4729, -0.0249],\n",
       "        ...,\n",
       "        [-0.4095, -1.1725, -0.0697,  ...,  0.4732,  0.9023,  1.8954],\n",
       "        [ 0.9798,  0.2345, -0.0410,  ...,  1.0442,  1.3068, -0.4143],\n",
       "        [-0.4071, -0.0628, -0.7700,  ..., -0.2350,  1.0591,  0.3958]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2e.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embedding_dims):\n",
    "        super(Attention, self).__init__()\n",
    "        self.embed_dim = embedding_dims\n",
    "#         进行线性变换   (N*64) *A * (N * 64) + b\n",
    "        self.bilinear = nn.Bilinear(self.embed_dim, self.embed_dim, 1)\n",
    "        self.att1 = nn.Linear(self.embed_dim * 2, self.embed_dim)   #128*64\n",
    "        self.att2 = nn.Linear(self.embed_dim, self.embed_dim)   # 64*64\n",
    "        self.att3 = nn.Linear(self.embed_dim, 1)   # 64*1\n",
    "        self.softmax = nn.Softmax(0)\n",
    "\n",
    "    def forward(self, node1, u_rep, num_neighs):\n",
    "        uv_reps = u_rep.repeat(num_neighs, 1)\n",
    "        x = torch.cat((node1, uv_reps), 1)\n",
    "        x = F.relu(self.att1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.att2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.att3(x)\n",
    "        att = F.softmax(x, dim=0)\n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5792, -0.6137,  1.0095, -0.6832,  0.2060, -1.2658,  1.1402,  0.4925,\n",
       "        -0.0717, -0.6354, -0.1669, -0.3147,  0.0313,  2.0650, -0.6001, -0.5757,\n",
       "         0.6481,  0.5077, -0.8498,  0.0419, -0.3678, -0.1515,  1.4606,  0.0768,\n",
       "        -0.0970, -1.1362,  0.4450, -0.0806, -0.5280, -1.7293, -0.1698,  0.0600,\n",
       "         0.3642,  0.1660,  1.0511, -0.2953,  0.5412,  1.5216,  0.9889,  0.2657,\n",
       "        -1.2285,  1.3287,  0.0948,  0.4276,  0.2429, -0.7598,  0.3844, -0.6880,\n",
       "        -0.4076, -0.0760,  0.1681,  1.5098,  0.0557,  1.8551, -0.4345,  0.5974,\n",
       "        -0.6801, -1.5289, -1.3795, -0.2088, -1.0530,  1.6212,  0.0908,  1.0095],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2e.weight[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UV_Aggregator\n",
    "\n",
    "UV_Aggregator(v2e, r2e, u2e, embed_dim, cuda=device, uv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UV_Aggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    item and user aggregator: for aggregating embeddings of neighbors (item/user aggreagator).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, v2e, r2e, u2e, embed_dim, cuda=\"cpu\", uv=True):\n",
    "        super(UV_Aggregator, self).__init__()\n",
    "        self.uv = uv\n",
    "        self.v2e = v2e\n",
    "        self.r2e = r2e\n",
    "        self.u2e = u2e\n",
    "        self.device = cuda\n",
    "        self.embed_dim = embed_dim  #64\n",
    "#         128 * 64\n",
    "        self.w_r1 = nn.Linear(self.embed_dim * 2, self.embed_dim)  \n",
    "#         64*64\n",
    "        self.w_r2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "#         64*1\n",
    "        self.att = Attention(self.embed_dim)\n",
    "        \n",
    "    def forward(self, nodes, history_uv, history_r):\n",
    "#         随机生成的embedding 矩阵    送到内存中\n",
    "        embed_matrix = torch.empty(len(history_uv), self.embed_dim, dtype=torch.float).to(self.device)\n",
    "        \n",
    "        for i in range(len(history_uv)):\n",
    "#             对于每一条记录\n",
    "            history = history_uv[i]\n",
    "#             每条记录的长度\n",
    "            num_histroy_item = len(history)\n",
    "#             标签\n",
    "            tmp_label = history_r[i]\n",
    "            \n",
    "            if self.uv == True:\n",
    "                # user component\n",
    "#                 去history个items的嵌入向量\n",
    "                e_uv = self.v2e.weight[history]\n",
    "#                   取history个user的嵌入向量\n",
    "                uv_rep = self.u2e.weight[nodes[i]]\n",
    "            else:\n",
    "                # item component\n",
    "                e_uv = self.u2e.weight[history]\n",
    "                uv_rep = self.v2e.weight[nodes[i]]\n",
    "#             将具体打分转化成向量\n",
    "            e_r = self.r2e.weight[tmp_label]\n",
    "#             user 和 item 向量拼接\n",
    "            x = torch.cat((e_uv, e_r), 1)\n",
    "            x = F.relu(self.w_r1(x))\n",
    "            o_history = F.relu(self.w_r2(x))\n",
    "#             求软注意力机制的 alpha\n",
    "            att_w = self.att(o_history, uv_rep, num_histroy_item)\n",
    "#             执行 mat1 和 mat2 的矩阵乘法.\n",
    "#             gv() 为两层神经网络\n",
    "            att_history = torch.mm(o_history.t(), att_w)\n",
    "            att_history = att_history.t()\n",
    "            \n",
    "#             att_history:  1*64\n",
    "            embed_matrix[i] = att_history\n",
    "        to_feats = embed_matrix\n",
    "        return to_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.8494e-36, -9.0072e+15,  7.1555e+22],\n",
       "        [ 5.5158e-14,  9.3853e+09,  8.8544e+14],\n",
       "        [ 4.9485e+04,  2.2034e-10,  2.5353e+30],\n",
       "        [ 6.4460e-44,  3.2647e+03,  2.1737e+05],\n",
       "        [ 8.4431e+08,  6.5613e-39,  8.3971e-32]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UV_Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UV_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features, embed_dim, history_uv_lists, history_r_lists, aggregator, cuda=\"cpu\", uv=True):\n",
    "        super(UV_Encoder, self).__init__()\n",
    "        \n",
    "        self.features = features\n",
    "        self.uv = uv\n",
    "        self.history_uv_lists = history_uv_lists\n",
    "        self.history_r_lists = history_r_lists\n",
    "        self.aggregator = aggregator\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = cuda\n",
    "        self.linear1 = nn.Linear(2 * self.embed_dim, self.embed_dim)  #\n",
    "        \n",
    "    def forward(self, nodes):\n",
    "        tmp_history_uv = []\n",
    "        tmp_history_r = []\n",
    "        for node in nodes:\n",
    "            tmp_history_uv.append(self.history_uv_lists[int(node)])\n",
    "            tmp_history_r.append(self.history_r_lists[int(node)])\n",
    "            \n",
    "#             邻居节点的向量表示\n",
    "        neigh_feats = self.aggregator.forward(nodes, tmp_history_uv, tmp_history_r)  # user-item network\n",
    "#             自身节点的嵌入\n",
    "        self_feats = self.features.weight[nodes]\n",
    "            # self-connection could be considered.\n",
    "        combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "#         线性变换\n",
    "        combined = F.relu(self.linear1(combined))\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social_Aggregator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Social_Aggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Social Aggregator: for aggregating embeddings of social neighbors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, u2e, embed_dim, cuda=\"cpu\"):\n",
    "        super(Social_Aggregator, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.device = cuda\n",
    "        self.u2e = u2e\n",
    "        self.embed_dim = embed_dim\n",
    "        self.att = Attention(self.embed_dim)\n",
    "        \n",
    "    def forward(self,nodes,to_neighs):\n",
    "        \n",
    "        embed_matrix = torch.empty(len(nodes), self.embed_dim, dtype=torch.float).to(self.device)\n",
    "#         遍历所有节点\n",
    "        for i in range(len(nodes)):\n",
    "#             所有邻居节点\n",
    "            tmp_adj = to_neighs[i]\n",
    "            num_neighs = len(tmp_adj)\n",
    "            # 邻居节点 嵌入\n",
    "            e_u = self.u2e.weight[list(tmp_adj)] # fast: user embedding \n",
    "            \n",
    "            #slow: item-space user latent factor (item aggregation)\n",
    "            #feature_neigbhors = self.features(torch.LongTensor(list(tmp_adj)).to(self.device))\n",
    "            #e_u = torch.t(feature_neigbhors)\n",
    "#             中心的user的向量表示\n",
    "            u_rep = self.u2e.weight[nodes[i]]\n",
    "#             通过用户和邻居的向量学习alpha\n",
    "            att_w = self.att(e_u, u_rep, num_neighs)\n",
    "#             Aggre_{neigbhors}\n",
    "            att_history = torch.mm(e_u.t(), att_w).t()\n",
    "            embed_matrix[i] = att_history\n",
    "        to_feats = embed_matrix\n",
    "\n",
    "        return to_feats\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social_Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Social_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features, embed_dim, social_adj_lists, aggregator, base_model=None, cuda=\"cpu\"):\n",
    "        super(Social_Encoder, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.social_adj_lists = social_adj_lists\n",
    "        self.aggregator = aggregator\n",
    "        if base_model != None:\n",
    "            self.base_model = base_model\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = cuda\n",
    "        self.linear1 = nn.Linear(2 * self.embed_dim, self.embed_dim)  #\n",
    "\n",
    "    def forward(self, nodes):\n",
    "\n",
    "        to_neighs = []\n",
    "        for node in nodes:\n",
    "            to_neighs.append(self.social_adj_lists[int(node)])\n",
    "        \n",
    "#         Aggre_{neigbhors}\n",
    "        neigh_feats = self.aggregator.forward(nodes, to_neighs)  # user-user network\n",
    "    \n",
    "#         这里的feature是user-item传递过来的user的嵌入词典\n",
    "        self_feats = self.features(torch.LongTensor(nodes.cpu().numpy())).to(self.device)\n",
    "        self_feats = self_feats.t()\n",
    "        \n",
    "        # self-connection could be considered.\n",
    "        combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "        combined = F.relu(self.linear1(combined))\n",
    "\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user feature\n",
    "    # features: item * rating\n",
    "agg_u_history = UV_Aggregator(v2e, r2e, u2e, embed_dim, cuda=device, uv=True)\n",
    "enc_u_history = UV_Encoder(u2e, embed_dim, history_u_lists, history_ur_lists, agg_u_history, cuda=device, uv=True)\n",
    "    # neighobrs\n",
    "agg_u_social = Social_Aggregator(lambda nodes: enc_u_history(nodes).t(), u2e, embed_dim, cuda=device)\n",
    "enc_u = Social_Encoder(lambda nodes: enc_u_history(nodes).t(), embed_dim, social_adj_lists, agg_u_social,\n",
    "                        base_model=enc_u_history, cuda=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item feature: user * rating\n",
    "agg_v_history = UV_Aggregator(v2e, r2e, u2e, embed_dim, cuda=device, uv=False)\n",
    "enc_v_history = UV_Encoder(v2e, embed_dim, history_v_lists, history_vr_lists, agg_v_history, cuda=device, uv=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  GraphRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRec(nn.Module):\n",
    "\n",
    "    def __init__(self, enc_u, enc_v_history, r2e):\n",
    "        super(GraphRec, self).__init__()\n",
    "        self.enc_u = enc_u\n",
    "        self.enc_v_history = enc_v_history\n",
    "        self.embed_dim = enc_u.embed_dim\n",
    "\n",
    "        self.w_ur1 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_ur2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_vr1 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_vr2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_uv1 = nn.Linear(self.embed_dim * 2, self.embed_dim)\n",
    "        self.w_uv2 = nn.Linear(self.embed_dim, 16)\n",
    "        self.w_uv3 = nn.Linear(16, 1)\n",
    "        self.r2e = r2e\n",
    "        self.bn1 = nn.BatchNorm1d(self.embed_dim, momentum=0.5)\n",
    "        self.bn2 = nn.BatchNorm1d(self.embed_dim, momentum=0.5)\n",
    "        self.bn3 = nn.BatchNorm1d(self.embed_dim, momentum=0.5)\n",
    "        self.bn4 = nn.BatchNorm1d(16, momentum=0.5)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, nodes_u, nodes_v):\n",
    "        embeds_u = self.enc_u(nodes_u)\n",
    "        embeds_v = self.enc_v_history(nodes_v)\n",
    "\n",
    "        x_u = F.relu(self.bn1(self.w_ur1(embeds_u)))\n",
    "        x_u = F.dropout(x_u, training=self.training)\n",
    "        x_u = self.w_ur2(x_u)\n",
    "        x_v = F.relu(self.bn2(self.w_vr1(embeds_v)))\n",
    "        x_v = F.dropout(x_v, training=self.training)\n",
    "        x_v = self.w_vr2(x_v)\n",
    "\n",
    "        x_uv = torch.cat((x_u, x_v), 1)\n",
    "        x = F.relu(self.bn3(self.w_uv1(x_uv)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.bn4(self.w_uv2(x)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        scores = self.w_uv3(x)\n",
    "        return scores.squeeze()\n",
    "\n",
    "    def loss(self, nodes_u, nodes_v, labels_list):\n",
    "        scores = self.forward(nodes_u, nodes_v)\n",
    "        return self.criterion(scores, labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, best_rmse, best_mae):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        batch_nodes_u, batch_nodes_v, labels_list = data\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(batch_nodes_u.to(device), batch_nodes_v.to(device), labels_list.to(device))\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.3f, The best rmse/mae: %.6f / %.6f' % (\n",
    "                epoch, i, running_loss / 100, best_rmse, best_mae))\n",
    "            running_loss = 0.0\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    tmp_pred = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for test_u, test_v, tmp_target in test_loader:\n",
    "            test_u, test_v, tmp_target = test_u.to(device), test_v.to(device), tmp_target.to(device)\n",
    "            val_output = model.forward(test_u, test_v)\n",
    "            tmp_pred.append(list(val_output.data.cpu().numpy()))\n",
    "            target.append(list(tmp_target.data.cpu().numpy()))\n",
    "    tmp_pred = np.array(sum(tmp_pred, []))\n",
    "    target = np.array(sum(target, []))\n",
    "    expected_rmse = sqrt(mean_squared_error(tmp_pred, target))\n",
    "    mae = mean_absolute_error(tmp_pred, target)\n",
    "    return expected_rmse, mae\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='Social Recommendation: GraphRec model')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, metavar='N', help='input batch size for training')\n",
    "    parser.add_argument('--embed_dim', type=int, default=64, metavar='N', help='embedding size')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR', help='learning rate')\n",
    "    parser.add_argument('--test_batch_size', type=int, default=1000, metavar='N', help='input batch size for testing')\n",
    "    parser.add_argument('--epochs', type=int, default=100, metavar='N', help='number of epochs to train')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    use_cuda = False\n",
    "    if torch.cuda.is_available():\n",
    "        use_cuda = True\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    embed_dim = args.embed_dim\n",
    "    dir_data = './data/toy_dataset'\n",
    "\n",
    "    path_data = dir_data + \".pickle\"\n",
    "    data_file = open(path_data, 'rb')\n",
    "    history_u_lists, history_ur_lists, history_v_lists, history_vr_lists, train_u, train_v, train_r, test_u, test_v, test_r, social_adj_lists, ratings_list = pickle.load(\n",
    "        data_file)\n",
    "\n",
    "\n",
    "    trainset = torch.utils.data.TensorDataset(torch.LongTensor(train_u), torch.LongTensor(train_v),\n",
    "                                              torch.FloatTensor(train_r))\n",
    "    testset = torch.utils.data.TensorDataset(torch.LongTensor(test_u), torch.LongTensor(test_v),\n",
    "                                             torch.FloatTensor(test_r))\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=args.test_batch_size, shuffle=True)\n",
    "    num_users = history_u_lists.__len__()\n",
    "    num_items = history_v_lists.__len__()\n",
    "    num_ratings = ratings_list.__len__()\n",
    "\n",
    "    u2e = nn.Embedding(num_users, embed_dim).to(device)\n",
    "    v2e = nn.Embedding(num_items, embed_dim).to(device)\n",
    "    r2e = nn.Embedding(num_ratings, embed_dim).to(device)\n",
    "\n",
    "    # user feature\n",
    "    # features: item * rating\n",
    "    # 先通过将user-item 进行 Agger  \n",
    "    agg_u_history = UV_Aggregator(v2e, r2e, u2e, embed_dim, cuda=device, uv=True)\n",
    "    \n",
    "    enc_u_history = UV_Encoder(u2e, embed_dim, history_u_lists, history_ur_lists, agg_u_history, cuda=device, uv=True)\n",
    "    # neighobrs\n",
    "    agg_u_social = Social_Aggregator(lambda nodes: enc_u_history(nodes).t(), u2e, embed_dim, cuda=device)\n",
    "    enc_u = Social_Encoder(lambda nodes: enc_u_history(nodes).t(), embed_dim, social_adj_lists, agg_u_social,\n",
    "                           base_model=enc_u_history, cuda=device)\n",
    "\n",
    "    # item feature: user * rating\n",
    "    agg_v_history = UV_Aggregator(v2e, r2e, u2e, embed_dim, cuda=device, uv=False)\n",
    "    enc_v_history = UV_Encoder(v2e, embed_dim, history_v_lists, history_vr_lists, agg_v_history, cuda=device, uv=False)\n",
    "\n",
    "    # model\n",
    "    graphrec = GraphRec(enc_u, enc_v_history, r2e).to(device)\n",
    "    optimizer = torch.optim.RMSprop(graphrec.parameters(), lr=args.lr, alpha=0.9)\n",
    "\n",
    "    best_rmse = 9999.0\n",
    "    best_mae = 9999.0\n",
    "    endure_count = 0\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "\n",
    "        train(graphrec, device, train_loader, optimizer, epoch, best_rmse, best_mae)\n",
    "        expected_rmse, mae = test(graphrec, device, test_loader)\n",
    "        # please add the validation set to tune the hyper-parameters based on your datasets.\n",
    "\n",
    "        # early stopping (no validation set in toy dataset)\n",
    "        if best_rmse > expected_rmse:\n",
    "            best_rmse = expected_rmse\n",
    "            best_mae = mae\n",
    "            endure_count = 0\n",
    "        else:\n",
    "            endure_count += 1\n",
    "        print(\"rmse: %.4f, mae:%.4f \" % (expected_rmse, mae))\n",
    "\n",
    "        if endure_count > 5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     0] loss: 0.105, The best rmse/mae: 9999.000000 / 9999.000000\n",
      "[1,   100] loss: 8.032, The best rmse/mae: 9999.000000 / 9999.000000\n",
      "rmse: 2.4514, mae:2.2948 \n",
      "[2,     0] loss: 0.057, The best rmse/mae: 2.451415 / 2.294840\n",
      "[2,   100] loss: 4.005, The best rmse/mae: 2.451415 / 2.294840\n",
      "rmse: 1.5091, mae:1.3406 \n",
      "[3,     0] loss: 0.019, The best rmse/mae: 1.509148 / 1.340588\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
