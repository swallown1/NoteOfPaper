{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "# from model import *\n",
    "# from utils import build_graph, Data, split_validation\n",
    "import pickle\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='sample', help='dataset name: diginetica/yoochoose1_4/yoochoose1_64/sample')\n",
    "parser.add_argument('--method', type=str, default='ggnn', help='ggnn/gat/gcn')\n",
    "parser.add_argument('--validation', action='store_true', help='validation')\n",
    "parser.add_argument('--epoch', type=int, default=30, help='number of epochs to train for')\n",
    "parser.add_argument('--batchSize', type=int, default=100, help='input batch size')\n",
    "parser.add_argument('--hiddenSize', type=int, default=100, help='hidden state size')\n",
    "parser.add_argument('--l2', type=float, default=1e-5, help='l2 penalty')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--step', type=int, default=1, help='gnn propogation steps')\n",
    "parser.add_argument('--nonhybrid', action='store_true', help='global preference')\n",
    "parser.add_argument('--lr_dc', type=float, default=0.1, help='learning rate decay rate')\n",
    "parser.add_argument('--lr_dc_step', type=int, default=3, help='the number of steps after which the learning rate decay')\n",
    "\n",
    "\n",
    "opt = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pickle.load(open('../SR-GNN-/datasets/' + opt.dataset + '/train.txt', 'rb'))\n",
    "test_data = pickle.load(open('../SR-GNN-/datasets/' + opt.dataset + '/test.txt', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2],\n",
       " [1],\n",
       " [4],\n",
       " [6],\n",
       " [8, 9],\n",
       " [8],\n",
       " [10, 11, 11],\n",
       " [10, 11],\n",
       " [10],\n",
       " [12],\n",
       " [14],\n",
       " [15, 16, 17],\n",
       " [15, 16],\n",
       " [15],\n",
       " [19],\n",
       " [20],\n",
       " [21],\n",
       " [22, 22, 23, 23, 23, 22, 23],\n",
       " [22, 22, 23, 23, 23, 22],\n",
       " [22, 22, 23, 23, 23],\n",
       " [22, 22, 23, 23],\n",
       " [22, 22, 23],\n",
       " [22, 22],\n",
       " [22],\n",
       " [24, 25, 26],\n",
       " [24, 25],\n",
       " [24],\n",
       " [1, 28, 3],\n",
       " [1, 28],\n",
       " [1],\n",
       " [29],\n",
       " [31, 32, 31, 31, 31],\n",
       " [31, 32, 31, 31],\n",
       " [31, 32, 31],\n",
       " [31, 32],\n",
       " [31],\n",
       " [33, 33],\n",
       " [33],\n",
       " [34, 34, 34, 34, 34],\n",
       " [34, 34, 34, 34],\n",
       " [34, 34, 34],\n",
       " [34, 34],\n",
       " [34],\n",
       " [12, 13, 12, 13, 35, 35, 12, 13],\n",
       " [12, 13, 12, 13, 35, 35, 12],\n",
       " [12, 13, 12, 13, 35, 35],\n",
       " [12, 13, 12, 13, 35],\n",
       " [12, 13, 12, 13],\n",
       " [12, 13, 12],\n",
       " [12, 13],\n",
       " [12],\n",
       " [36],\n",
       " [37, 37, 37],\n",
       " [37, 37],\n",
       " [37],\n",
       " [39],\n",
       " [40],\n",
       " [42],\n",
       " [43, 44],\n",
       " [43],\n",
       " [45, 46, 47],\n",
       " [45, 46],\n",
       " [45],\n",
       " [35, 35],\n",
       " [35],\n",
       " [20],\n",
       " [46],\n",
       " [46],\n",
       " [10, 10],\n",
       " [10],\n",
       " [3],\n",
       " [13],\n",
       " [50],\n",
       " [51, 52],\n",
       " [51],\n",
       " [53, 54, 54],\n",
       " [53, 54],\n",
       " [53],\n",
       " [55, 56],\n",
       " [55],\n",
       " [7],\n",
       " [58, 59],\n",
       " [58],\n",
       " [60],\n",
       " [61, 61, 61, 61, 61, 61],\n",
       " [61, 61, 61, 61, 61],\n",
       " [61, 61, 61, 61],\n",
       " [61, 61, 61],\n",
       " [61, 61],\n",
       " [61],\n",
       " [38],\n",
       " [62, 62, 63, 62],\n",
       " [62, 62, 63],\n",
       " [62, 62],\n",
       " [62],\n",
       " [7],\n",
       " [64],\n",
       " [43, 65, 66, 5, 4, 67],\n",
       " [43, 65, 66, 5, 4],\n",
       " [43, 65, 66, 5],\n",
       " [43, 65, 66],\n",
       " [43, 65],\n",
       " [43],\n",
       " [14, 68, 69, 14, 14, 68, 14, 69],\n",
       " [14, 68, 69, 14, 14, 68, 14],\n",
       " [14, 68, 69, 14, 14, 68],\n",
       " [14, 68, 69, 14, 14],\n",
       " [14, 68, 69, 14],\n",
       " [14, 68, 69],\n",
       " [14, 68],\n",
       " [14],\n",
       " [70, 60, 70, 70],\n",
       " [70, 60, 70],\n",
       " [70, 60],\n",
       " [70],\n",
       " [71],\n",
       " [72],\n",
       " [73, 74, 73, 75, 73, 73],\n",
       " [73, 74, 73, 75, 73],\n",
       " [73, 74, 73, 75],\n",
       " [73, 74, 73],\n",
       " [73, 74],\n",
       " [73],\n",
       " [76],\n",
       " [44, 78],\n",
       " [44],\n",
       " [79, 79],\n",
       " [79],\n",
       " [80],\n",
       " [82],\n",
       " [83],\n",
       " [84],\n",
       " [85],\n",
       " [86, 86, 87, 86],\n",
       " [86, 86, 87],\n",
       " [86, 86],\n",
       " [86],\n",
       " [88],\n",
       " [81, 81],\n",
       " [81],\n",
       " [89],\n",
       " [24],\n",
       " [90],\n",
       " [92, 92],\n",
       " [92],\n",
       " [44, 93, 94],\n",
       " [44, 93],\n",
       " [44],\n",
       " [77, 77],\n",
       " [77],\n",
       " [67],\n",
       " [91],\n",
       " [96, 74, 97, 74, 96, 98],\n",
       " [96, 74, 97, 74, 96],\n",
       " [96, 74, 97, 74],\n",
       " [96, 74, 97],\n",
       " [96, 74],\n",
       " [96],\n",
       " [76],\n",
       " [99],\n",
       " [100],\n",
       " [101],\n",
       " [103, 104, 103, 105, 105, 103],\n",
       " [103, 104, 103, 105, 105],\n",
       " [103, 104, 103, 105],\n",
       " [103, 104, 103],\n",
       " [103, 104],\n",
       " [103],\n",
       " [107, 108, 108, 107],\n",
       " [107, 108, 108],\n",
       " [107, 108],\n",
       " [107],\n",
       " [109],\n",
       " [110],\n",
       " [44],\n",
       " [111],\n",
       " [112, 112],\n",
       " [112],\n",
       " [113],\n",
       " [115],\n",
       " [116, 116, 116, 116],\n",
       " [116, 116, 116],\n",
       " [116, 116],\n",
       " [116],\n",
       " [58, 117],\n",
       " [58],\n",
       " [70, 60],\n",
       " [70],\n",
       " [108, 118],\n",
       " [108],\n",
       " [15],\n",
       " [119, 119],\n",
       " [119],\n",
       " [27, 27, 27, 120],\n",
       " [27, 27, 27],\n",
       " [27, 27],\n",
       " [27],\n",
       " [58],\n",
       " [58],\n",
       " [122],\n",
       " [123, 124],\n",
       " [123],\n",
       " [125],\n",
       " [127, 127],\n",
       " [127],\n",
       " [7, 57, 57],\n",
       " [7, 57],\n",
       " [7],\n",
       " [128, 128, 128, 128, 128],\n",
       " [128, 128, 128, 128],\n",
       " [128, 128, 128],\n",
       " [128, 128],\n",
       " [128],\n",
       " [129, 130, 130],\n",
       " [129, 130],\n",
       " [129],\n",
       " [131, 97, 132, 133],\n",
       " [131, 97, 132],\n",
       " [131, 97],\n",
       " [131],\n",
       " [133, 97, 97, 132, 134, 134],\n",
       " [133, 97, 97, 132, 134],\n",
       " [133, 97, 97, 132],\n",
       " [133, 97, 97],\n",
       " [133, 97],\n",
       " [133],\n",
       " [135, 136, 135],\n",
       " [135, 136],\n",
       " [135],\n",
       " [1, 1, 3],\n",
       " [1, 1],\n",
       " [1],\n",
       " [138, 138],\n",
       " [138],\n",
       " [139, 18, 18],\n",
       " [139, 18],\n",
       " [139],\n",
       " [72],\n",
       " [78],\n",
       " [141, 141, 141, 141],\n",
       " [141, 141, 141],\n",
       " [141, 141],\n",
       " [141],\n",
       " [142, 143, 143, 142, 142, 142, 143, 143, 142, 143, 142, 143, 142],\n",
       " [142, 143, 143, 142, 142, 142, 143, 143, 142, 143, 142, 143],\n",
       " [142, 143, 143, 142, 142, 142, 143, 143, 142, 143, 142],\n",
       " [142, 143, 143, 142, 142, 142, 143, 143, 142, 143],\n",
       " [142, 143, 143, 142, 142, 142, 143, 143, 142],\n",
       " [142, 143, 143, 142, 142, 142, 143, 143],\n",
       " [142, 143, 143, 142, 142, 142, 143],\n",
       " [142, 143, 143, 142, 142, 142],\n",
       " [142, 143, 143, 142, 142],\n",
       " [142, 143, 143, 142],\n",
       " [142, 143, 143],\n",
       " [142, 143],\n",
       " [142],\n",
       " [69],\n",
       " [144],\n",
       " [145],\n",
       " [86],\n",
       " [78, 94],\n",
       " [78],\n",
       " [47, 40],\n",
       " [47],\n",
       " [87],\n",
       " [93, 8, 5],\n",
       " [93, 8],\n",
       " [93],\n",
       " [40, 40],\n",
       " [40],\n",
       " [147, 147, 147, 147],\n",
       " [147, 147, 147],\n",
       " [147, 147],\n",
       " [147],\n",
       " [124],\n",
       " [148, 87],\n",
       " [148],\n",
       " [149, 149],\n",
       " [149],\n",
       " [150, 151, 150, 151, 150, 150, 150, 151, 150, 152, 152],\n",
       " [150, 151, 150, 151, 150, 150, 150, 151, 150, 152],\n",
       " [150, 151, 150, 151, 150, 150, 150, 151, 150],\n",
       " [150, 151, 150, 151, 150, 150, 150, 151],\n",
       " [150, 151, 150, 151, 150, 150, 150],\n",
       " [150, 151, 150, 151, 150, 150],\n",
       " [150, 151, 150, 151, 150],\n",
       " [150, 151, 150, 151],\n",
       " [150, 151, 150],\n",
       " [150, 151],\n",
       " [150],\n",
       " [153, 11, 11, 107, 108, 118],\n",
       " [153, 11, 11, 107, 108],\n",
       " [153, 11, 11, 107],\n",
       " [153, 11, 11],\n",
       " [153, 11],\n",
       " [153],\n",
       " [154],\n",
       " [121, 120],\n",
       " [121],\n",
       " [113],\n",
       " [40],\n",
       " [157],\n",
       " [137, 136],\n",
       " [137],\n",
       " [158, 159, 159, 159, 53, 159],\n",
       " [158, 159, 159, 159, 53],\n",
       " [158, 159, 159, 159],\n",
       " [158, 159, 159],\n",
       " [158, 159],\n",
       " [158],\n",
       " [52, 52],\n",
       " [52],\n",
       " [160],\n",
       " [161],\n",
       " [19],\n",
       " [52, 52],\n",
       " [52],\n",
       " [162],\n",
       " [163, 163, 163, 123, 50, 75, 123],\n",
       " [163, 163, 163, 123, 50, 75],\n",
       " [163, 163, 163, 123, 50],\n",
       " [163, 163, 163, 123],\n",
       " [163, 163, 163],\n",
       " [163, 163],\n",
       " [163],\n",
       " [53],\n",
       " [85],\n",
       " [112, 56, 112],\n",
       " [112, 56],\n",
       " [112],\n",
       " [54, 165, 53, 164],\n",
       " [54, 165, 53],\n",
       " [54, 165],\n",
       " [54],\n",
       " [166],\n",
       " [167],\n",
       " [154, 155],\n",
       " [154],\n",
       " [168, 169],\n",
       " [168],\n",
       " [169, 171, 172, 173],\n",
       " [169, 171, 172],\n",
       " [169, 171],\n",
       " [169],\n",
       " [174, 174, 174, 146],\n",
       " [174, 174, 174],\n",
       " [174, 174],\n",
       " [174],\n",
       " [155, 155, 154],\n",
       " [155, 155],\n",
       " [155],\n",
       " [175, 175, 175, 175],\n",
       " [175, 175, 175],\n",
       " [175, 175],\n",
       " [175],\n",
       " [176],\n",
       " [177, 177, 177],\n",
       " [177, 177],\n",
       " [177],\n",
       " [178, 178],\n",
       " [178],\n",
       " [66],\n",
       " [180, 180],\n",
       " [180],\n",
       " [181, 181],\n",
       " [181],\n",
       " [182],\n",
       " [65],\n",
       " [183, 183],\n",
       " [183],\n",
       " [19, 19],\n",
       " [19],\n",
       " [18, 15],\n",
       " [18],\n",
       " [37],\n",
       " [132],\n",
       " [184, 184],\n",
       " [184],\n",
       " [185, 185, 185, 185, 185],\n",
       " [185, 185, 185, 185],\n",
       " [185, 185, 185],\n",
       " [185, 185],\n",
       " [185],\n",
       " [24, 186],\n",
       " [24],\n",
       " [17, 16, 187, 187],\n",
       " [17, 16, 187],\n",
       " [17, 16],\n",
       " [17],\n",
       " [85, 29, 85],\n",
       " [85, 29],\n",
       " [85],\n",
       " [188],\n",
       " [165, 164],\n",
       " [165],\n",
       " [7, 6, 7, 6],\n",
       " [7, 6, 7],\n",
       " [7, 6],\n",
       " [7],\n",
       " [189],\n",
       " [190, 190, 190],\n",
       " [190, 190],\n",
       " [190],\n",
       " [187, 49],\n",
       " [187],\n",
       " [191],\n",
       " [26],\n",
       " [107, 25],\n",
       " [107],\n",
       " [192],\n",
       " [91, 90],\n",
       " [91],\n",
       " [193, 193, 193, 193],\n",
       " [193, 193, 193],\n",
       " [193, 193],\n",
       " [193],\n",
       " [194, 194],\n",
       " [194],\n",
       " [181, 195],\n",
       " [181],\n",
       " [196, 196, 196, 196],\n",
       " [196, 196, 196],\n",
       " [196, 196],\n",
       " [196],\n",
       " [181],\n",
       " [70],\n",
       " [30],\n",
       " [87],\n",
       " [27, 120, 121],\n",
       " [27, 120],\n",
       " [27],\n",
       " [198, 172, 172, 199],\n",
       " [198, 172, 172],\n",
       " [198, 172],\n",
       " [198],\n",
       " [110, 110],\n",
       " [110],\n",
       " [200, 200],\n",
       " [200],\n",
       " [184, 184, 184, 184, 184, 201, 201, 184],\n",
       " [184, 184, 184, 184, 184, 201, 201],\n",
       " [184, 184, 184, 184, 184, 201],\n",
       " [184, 184, 184, 184, 184],\n",
       " [184, 184, 184, 184],\n",
       " [184, 184, 184],\n",
       " [184, 184],\n",
       " [184],\n",
       " [202, 202, 202, 202, 202, 202, 202, 202],\n",
       " [202, 202, 202, 202, 202, 202, 202],\n",
       " [202, 202, 202, 202, 202, 202],\n",
       " [202, 202, 202, 202, 202],\n",
       " [202, 202, 202, 202],\n",
       " [202, 202, 202],\n",
       " [202, 202],\n",
       " [202],\n",
       " [203],\n",
       " [184, 204, 184, 184],\n",
       " [184, 204, 184],\n",
       " [184, 204],\n",
       " [184],\n",
       " [157, 111],\n",
       " [157],\n",
       " [43, 94, 93, 93, 43],\n",
       " [43, 94, 93, 93],\n",
       " [43, 94, 93],\n",
       " [43, 94],\n",
       " [43],\n",
       " [205, 205, 206],\n",
       " [205, 205],\n",
       " [205],\n",
       " [85, 29],\n",
       " [85],\n",
       " [18],\n",
       " [88],\n",
       " [207, 207, 207, 207, 207, 207, 207, 207, 207, 207, 207],\n",
       " [207, 207, 207, 207, 207, 207, 207, 207, 207, 207],\n",
       " [207, 207, 207, 207, 207, 207, 207, 207, 207],\n",
       " [207, 207, 207, 207, 207, 207, 207, 207],\n",
       " [207, 207, 207, 207, 207, 207, 207],\n",
       " [207, 207, 207, 207, 207, 207],\n",
       " [207, 207, 207, 207, 207],\n",
       " [207, 207, 207, 207],\n",
       " [207, 207, 207],\n",
       " [207, 207],\n",
       " [207],\n",
       " [208, 208, 208, 208],\n",
       " [208, 208, 208],\n",
       " [208, 208],\n",
       " [208],\n",
       " [101],\n",
       " [209, 209, 209, 209, 209, 209],\n",
       " [209, 209, 209, 209, 209],\n",
       " [209, 209, 209, 209],\n",
       " [209, 209, 209],\n",
       " [209, 209],\n",
       " [209],\n",
       " [48],\n",
       " [136, 137, 136, 137],\n",
       " [136, 137, 136],\n",
       " [136, 137],\n",
       " [136],\n",
       " [180, 189],\n",
       " [180],\n",
       " [203],\n",
       " [210],\n",
       " [205, 205, 205],\n",
       " [205, 205],\n",
       " [205],\n",
       " [27],\n",
       " [69, 68, 69],\n",
       " [69, 68],\n",
       " [69],\n",
       " [131],\n",
       " [211, 211, 52, 109, 187],\n",
       " [211, 211, 52, 109],\n",
       " [211, 211, 52],\n",
       " [211, 211],\n",
       " [211],\n",
       " [149, 149],\n",
       " [149],\n",
       " [18, 17],\n",
       " [18],\n",
       " [212, 212, 212],\n",
       " [212, 212],\n",
       " [212],\n",
       " [183],\n",
       " [136],\n",
       " [214, 214],\n",
       " [214],\n",
       " [215],\n",
       " [216],\n",
       " [101],\n",
       " [139, 139],\n",
       " [139],\n",
       " [217],\n",
       " [60, 164],\n",
       " [60],\n",
       " [30],\n",
       " [218],\n",
       " [219, 219, 219],\n",
       " [219, 219],\n",
       " [219],\n",
       " [146],\n",
       " [59, 58, 59, 6],\n",
       " [59, 58, 59],\n",
       " [59, 58],\n",
       " [59],\n",
       " [220, 220],\n",
       " [220],\n",
       " [218, 172, 218, 173, 173, 218, 218],\n",
       " [218, 172, 218, 173, 173, 218],\n",
       " [218, 172, 218, 173, 173],\n",
       " [218, 172, 218, 173],\n",
       " [218, 172, 218],\n",
       " [218, 172],\n",
       " [218],\n",
       " [213, 221],\n",
       " [213],\n",
       " [222, 223, 224, 225, 226],\n",
       " [222, 223, 224, 225],\n",
       " [222, 223, 224],\n",
       " [222, 223],\n",
       " [222],\n",
       " [71],\n",
       " [67, 157, 12, 140, 44, 157],\n",
       " [67, 157, 12, 140, 44],\n",
       " [67, 157, 12, 140],\n",
       " [67, 157, 12],\n",
       " [67, 157],\n",
       " [67],\n",
       " [227, 227, 227, 228, 228, 228, 228, 227],\n",
       " [227, 227, 227, 228, 228, 228, 228],\n",
       " [227, 227, 227, 228, 228, 228],\n",
       " [227, 227, 227, 228, 228],\n",
       " [227, 227, 227, 228],\n",
       " [227, 227, 227],\n",
       " [227, 227],\n",
       " [227],\n",
       " [21],\n",
       " [38],\n",
       " [102, 86, 146, 87],\n",
       " [102, 86, 146],\n",
       " [102, 86],\n",
       " [102],\n",
       " [182],\n",
       " [221],\n",
       " [58],\n",
       " [166, 166],\n",
       " [166],\n",
       " [60, 70],\n",
       " [60],\n",
       " [231, 231, 231, 231],\n",
       " [231, 231, 231],\n",
       " [231, 231],\n",
       " [231],\n",
       " [232, 233, 232, 234, 232],\n",
       " [232, 233, 232, 234],\n",
       " [232, 233, 232],\n",
       " [232, 233],\n",
       " [232],\n",
       " [107, 118, 108, 118],\n",
       " [107, 118, 108],\n",
       " [107, 118],\n",
       " [107],\n",
       " [19, 19],\n",
       " [19],\n",
       " [16],\n",
       " [105, 104, 104, 104],\n",
       " [105, 104, 104],\n",
       " [105, 104],\n",
       " [105],\n",
       " [8],\n",
       " [173, 84],\n",
       " [173],\n",
       " [25, 121, 27, 121],\n",
       " [25, 121, 27],\n",
       " [25, 121],\n",
       " [25],\n",
       " [237],\n",
       " [56, 112, 181],\n",
       " [56, 112],\n",
       " [56],\n",
       " [35, 35, 35, 35],\n",
       " [35, 35, 35],\n",
       " [35, 35],\n",
       " [35],\n",
       " [191],\n",
       " [109],\n",
       " [28],\n",
       " [239, 240],\n",
       " [239],\n",
       " [216],\n",
       " [201, 204, 204, 201, 204, 201, 204, 204],\n",
       " [201, 204, 204, 201, 204, 201, 204],\n",
       " [201, 204, 204, 201, 204, 201],\n",
       " [201, 204, 204, 201, 204],\n",
       " [201, 204, 204, 201],\n",
       " [201, 204, 204],\n",
       " [201, 204],\n",
       " [201],\n",
       " [75, 131, 132],\n",
       " [75, 131],\n",
       " [75],\n",
       " [241, 241, 241, 241],\n",
       " [241, 241, 241],\n",
       " [241, 241],\n",
       " [241],\n",
       " [78, 44, 78, 94, 43, 94, 78],\n",
       " [78, 44, 78, 94, 43, 94],\n",
       " [78, 44, 78, 94, 43],\n",
       " [78, 44, 78, 94],\n",
       " [78, 44, 78],\n",
       " [78, 44],\n",
       " [78],\n",
       " [165, 164, 53, 164],\n",
       " [165, 164, 53],\n",
       " [165, 164],\n",
       " [165],\n",
       " [242, 242],\n",
       " [242],\n",
       " [51],\n",
       " [39],\n",
       " [123],\n",
       " [44, 94],\n",
       " [44],\n",
       " [3],\n",
       " [49],\n",
       " [199, 78, 243, 65, 178, 168, 199, 169],\n",
       " [199, 78, 243, 65, 178, 168, 199],\n",
       " [199, 78, 243, 65, 178, 168],\n",
       " [199, 78, 243, 65, 178],\n",
       " [199, 78, 243, 65],\n",
       " [199, 78, 243],\n",
       " [199, 78],\n",
       " [199],\n",
       " [25, 118],\n",
       " [25],\n",
       " [28, 244],\n",
       " [28],\n",
       " [245],\n",
       " [49],\n",
       " [41],\n",
       " [117, 7, 246],\n",
       " [117, 7],\n",
       " [117],\n",
       " [247],\n",
       " [17],\n",
       " [148, 249, 250, 250],\n",
       " [148, 249, 250],\n",
       " [148, 249],\n",
       " [148],\n",
       " [118, 107],\n",
       " [118],\n",
       " [9],\n",
       " [140, 44, 140, 44, 140],\n",
       " [140, 44, 140, 44],\n",
       " [140, 44, 140],\n",
       " [140, 44],\n",
       " [140],\n",
       " [236, 236, 236],\n",
       " [236, 236],\n",
       " [236],\n",
       " [251],\n",
       " [247],\n",
       " [111, 94],\n",
       " [111],\n",
       " [36],\n",
       " [252, 252],\n",
       " [252],\n",
       " [216, 216],\n",
       " [216],\n",
       " [253, 254],\n",
       " [253],\n",
       " [240, 129, 129],\n",
       " [240, 129],\n",
       " [240],\n",
       " [255],\n",
       " [253, 254, 253, 254, 253, 253, 254, 253, 254, 254, 253],\n",
       " [253, 254, 253, 254, 253, 253, 254, 253, 254, 254],\n",
       " [253, 254, 253, 254, 253, 253, 254, 253, 254],\n",
       " [253, 254, 253, 254, 253, 253, 254, 253],\n",
       " [253, 254, 253, 254, 253, 253, 254],\n",
       " [253, 254, 253, 254, 253, 253],\n",
       " [253, 254, 253, 254, 253],\n",
       " [253, 254, 253, 254],\n",
       " [253, 254, 253],\n",
       " [253, 254],\n",
       " [253],\n",
       " [7, 57],\n",
       " [7],\n",
       " [218, 77, 77, 216],\n",
       " [218, 77, 77],\n",
       " [218, 77],\n",
       " [218],\n",
       " [221, 82],\n",
       " [221],\n",
       " [228, 82],\n",
       " [228],\n",
       " [50, 73],\n",
       " [50],\n",
       " [256],\n",
       " [257],\n",
       " [60],\n",
       " [30, 85, 85, 85, 85, 63, 63],\n",
       " [30, 85, 85, 85, 85, 63],\n",
       " [30, 85, 85, 85, 85],\n",
       " [30, 85, 85, 85],\n",
       " [30, 85, 85],\n",
       " [30, 85],\n",
       " [30],\n",
       " [258, 258],\n",
       " [258],\n",
       " [150, 151, 150, 151],\n",
       " [150, 151, 150],\n",
       " [150, 151],\n",
       " [150],\n",
       " [9, 66],\n",
       " [9],\n",
       " [244],\n",
       " [180],\n",
       " [243, 243, 243, 243],\n",
       " [243, 243, 243],\n",
       " [243, 243],\n",
       " [243],\n",
       " [260, 261, 260, 260, 261, 261, 260, 261, 261],\n",
       " [260, 261, 260, 260, 261, 261, 260, 261],\n",
       " [260, 261, 260, 260, 261, 261, 260],\n",
       " [260, 261, 260, 260, 261, 261],\n",
       " [260, 261, 260, 260, 261],\n",
       " [260, 261, 260, 260],\n",
       " [260, 261, 260],\n",
       " [260, 261],\n",
       " [260],\n",
       " [18, 15],\n",
       " [18],\n",
       " [164],\n",
       " [215, 215, 262],\n",
       " [215, 215],\n",
       " [215],\n",
       " [119],\n",
       " [136, 135],\n",
       " [136],\n",
       " [264, 264, 264, 264],\n",
       " [264, 264, 264],\n",
       " [264, 264],\n",
       " [264],\n",
       " [16, 16],\n",
       " [16],\n",
       " [265, 33],\n",
       " [265],\n",
       " [106, 106, 104, 104, 104, 105, 104],\n",
       " [106, 106, 104, 104, 104, 105],\n",
       " [106, 106, 104, 104, 104],\n",
       " [106, 106, 104, 104],\n",
       " [106, 106, 104],\n",
       " [106, 106],\n",
       " [106],\n",
       " [106, 104],\n",
       " [106],\n",
       " [122, 266],\n",
       " [122],\n",
       " [187],\n",
       " [40],\n",
       " [262, 265],\n",
       " [262],\n",
       " [267],\n",
       " [98],\n",
       " [57, 57, 4],\n",
       " [57, 57],\n",
       " [57],\n",
       " [181],\n",
       " [33],\n",
       " [66, 199, 66, 259, 259, 199],\n",
       " [66, 199, 66, 259, 259],\n",
       " [66, 199, 66, 259],\n",
       " [66, 199, 66],\n",
       " [66, 199],\n",
       " [66],\n",
       " [160],\n",
       " [269, 269, 269, 269, 269, 269],\n",
       " [269, 269, 269, 269, 269],\n",
       " [269, 269, 269, 269],\n",
       " [269, 269, 269],\n",
       " [269, 269],\n",
       " [269],\n",
       " [45],\n",
       " [24, 24, 24, 24, 24],\n",
       " [24, 24, 24, 24],\n",
       " [24, 24, 24],\n",
       " [24, 24],\n",
       " [24],\n",
       " [105, 270],\n",
       " [105],\n",
       " [271, 272],\n",
       " [271],\n",
       " [240, 122, 130],\n",
       " [240, 122],\n",
       " [240],\n",
       " [113],\n",
       " [139, 18, 273],\n",
       " [139, 18],\n",
       " [139],\n",
       " [131, 134, 97],\n",
       " [131, 134],\n",
       " [131],\n",
       " [40],\n",
       " [129],\n",
       " [146, 86],\n",
       " [146],\n",
       " [229],\n",
       " [93, 43, 178],\n",
       " [93, 43],\n",
       " [93],\n",
       " [180],\n",
       " [33, 262, 145],\n",
       " [33, 262],\n",
       " [33],\n",
       " [274, 274, 274, 274],\n",
       " [274, 274, 274],\n",
       " [274, 274],\n",
       " [274],\n",
       " [139],\n",
       " [21, 229],\n",
       " [21],\n",
       " [27, 121],\n",
       " [27],\n",
       " [131],\n",
       " [260],\n",
       " [273],\n",
       " [56, 181, 56, 195, 55, 195, 55, 195, 56, 181],\n",
       " [56, 181, 56, 195, 55, 195, 55, 195, 56],\n",
       " [56, 181, 56, 195, 55, 195, 55, 195],\n",
       " [56, 181, 56, 195, 55, 195, 55],\n",
       " [56, 181, 56, 195, 55, 195],\n",
       " [56, 181, 56, 195, 55],\n",
       " [56, 181, 56, 195],\n",
       " [56, 181, 56],\n",
       " [56, 181],\n",
       " [56],\n",
       " [224,\n",
       "  222,\n",
       "  225,\n",
       "  223,\n",
       "  226,\n",
       "  223,\n",
       "  222,\n",
       "  223,\n",
       "  226,\n",
       "  223,\n",
       "  225,\n",
       "  223,\n",
       "  223,\n",
       "  223,\n",
       "  225,\n",
       "  222],\n",
       " [224, 222, 225, 223, 226, 223, 222, 223, 226, 223, 225, 223, 223, 223, 225],\n",
       " [224, 222, 225, 223, 226, 223, 222, 223, 226, 223, 225, 223, 223, 223],\n",
       " [224, 222, 225, 223, 226, 223, 222, 223, 226, 223, 225, 223, 223],\n",
       " [224, 222, 225, 223, 226, 223, 222, 223, 226, 223, 225, 223],\n",
       " [224, 222, 225, 223, 226, 223, 222, 223, 226, 223, 225],\n",
       " [224, 222, 225, 223, 226, 223, 222, 223, 226, 223],\n",
       " [224, 222, 225, 223, 226, 223, 222, 223, 226],\n",
       " [224, 222, 225, 223, 226, 223, 222, 223],\n",
       " [224, 222, 225, 223, 226, 223, 222],\n",
       " [224, 222, 225, 223, 226, 223],\n",
       " [224, 222, 225, 223, 226],\n",
       " [224, 222, 225, 223],\n",
       " [224, 222, 225],\n",
       " [224, 222],\n",
       " [224],\n",
       " [21],\n",
       " [278, 278, 278, 278, 278, 278],\n",
       " [278, 278, 278, 278, 278],\n",
       " [278, 278, 278, 278],\n",
       " [278, 278, 278],\n",
       " [278, 278],\n",
       " [278],\n",
       " [258],\n",
       " [245, 245],\n",
       " [245],\n",
       " [217],\n",
       " [190],\n",
       " [122, 130, 240, 240, 240, 240, 240],\n",
       " [122, 130, 240, 240, 240, 240],\n",
       " [122, 130, 240, 240, 240],\n",
       " [122, 130, 240, 240],\n",
       " [122, 130, 240],\n",
       " [122, 130],\n",
       " [122],\n",
       " [208],\n",
       " [183, 66, 199, 4],\n",
       " [183, 66, 199],\n",
       " [183, 66],\n",
       " [183],\n",
       " [279, 279],\n",
       " [279],\n",
       " [260, 256, 248],\n",
       " [260, 256],\n",
       " [260],\n",
       " [211],\n",
       " [63],\n",
       " [280, 202, 280, 280, 280],\n",
       " [280, 202, 280, 280],\n",
       " [280, 202, 280],\n",
       " [280, 202],\n",
       " [280],\n",
       " [43, 281, 199, 281, 43, 238],\n",
       " [43, 281, 199, 281, 43],\n",
       " [43, 281, 199, 281],\n",
       " [43, 281, 199],\n",
       " [43, 281],\n",
       " [43],\n",
       " [282, 282, 282, 282, 282],\n",
       " [282, 282, 282, 282],\n",
       " [282, 282, 282],\n",
       " [282, 282],\n",
       " [282],\n",
       " [93, 5, 264],\n",
       " [93, 5],\n",
       " [93],\n",
       " [1, 244, 161],\n",
       " [1, 244],\n",
       " [1],\n",
       " [24],\n",
       " [260, 247, 276, 260, 260, 276, 247, 248, 278, 260, 188],\n",
       " [260, 247, 276, 260, 260, 276, 247, 248, 278, 260],\n",
       " [260, 247, 276, 260, 260, 276, 247, 248, 278],\n",
       " [260, 247, 276, 260, 260, 276, 247, 248],\n",
       " [260, 247, 276, 260, 260, 276, 247],\n",
       " [260, 247, 276, 260, 260, 276],\n",
       " [260, 247, 276, 260, 260],\n",
       " [260, 247, 276, 260],\n",
       " [260, 247, 276],\n",
       " [260, 247],\n",
       " [260],\n",
       " [80],\n",
       " [276],\n",
       " [283, 55, 283, 283],\n",
       " [283, 55, 283],\n",
       " [283, 55],\n",
       " [283],\n",
       " [284],\n",
       " [285],\n",
       " [211],\n",
       " [198],\n",
       " [284, 249],\n",
       " [284],\n",
       " [45],\n",
       " [286, 286],\n",
       " [286],\n",
       " [218],\n",
       " [98, 124, 98],\n",
       " [98, 124],\n",
       " [98],\n",
       " [78, 111],\n",
       " [78],\n",
       " [17],\n",
       " [106],\n",
       " [171, 77, 242, 260, 248, 260, 247, 248],\n",
       " [171, 77, 242, 260, 248, 260, 247],\n",
       " [171, 77, 242, 260, 248, 260],\n",
       " [171, 77, 242, 260, 248],\n",
       " [171, 77, 242, 260],\n",
       " [171, 77, 242],\n",
       " [171, 77],\n",
       " [171],\n",
       " [288, 288, 285, 285, 288, 251, 251, 288],\n",
       " [288, 288, 285, 285, 288, 251, 251],\n",
       " [288, 288, 285, 285, 288, 251],\n",
       " [288, 288, 285, 285, 288],\n",
       " [288, 288, 285, 285],\n",
       " [288, 288, 285],\n",
       " [288, 288],\n",
       " [288],\n",
       " [289],\n",
       " ...]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_masks(all_usr_pois, item_tail): #输入：将所有输入序列all_usr_pois, 末尾补全的数据item_tail\n",
    "    us_lens = [len(upois) for upois in all_usr_pois] #每一个输入序列的长度的列表\n",
    "    len_max = max(us_lens)  #得到输入序列的最大长度\n",
    "    us_pois = [upois + item_tail * (len_max - le) for upois, le in zip(all_usr_pois, us_lens)] #将所有输入序列按照最长长度尾部补全 item_tail\n",
    "    us_msks = [[1] * le + [0] * (len_max - le) for le in us_lens] #有序列的位置是[1],没有动作序列的位置是[0]\n",
    "    return us_pois, us_msks, len_max  #输出：补全0后的序列us_pois, 面罩序列us_msks, 最大序列长度len_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = train_data[0] \n",
    "# 补全0后的序列us_pois, 面罩序列us_msks, 最大序列长度len_max\n",
    "inputs, mask, len_max = data_masks(inputs, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, data, method='ggnn',  shuffle=False):\n",
    "        inputs = data[0]   #输入序列的列表\n",
    "        #详见函数 ---> data_masks() \n",
    "        #这个函数使得所有会话按照最长的长度补0了！\n",
    "        inputs, mask, len_max = data_masks(inputs, [0])  \n",
    "       \n",
    "        self.inputs = np.asarray(inputs)  #补全0后的输入序列，并转化成array()\n",
    "        self.mask = np.asarray(mask)     #面罩序列，并转化成array()\n",
    "        self.len_max = len_max    #最大序列长度\n",
    "        self.targets = np.asarray(data[1])  #预测的序列的列表  也就是标签\n",
    "        self.length = len(inputs)  #输入样本的大小\n",
    "        self.shuffle = shuffle   #是否打乱数据\n",
    "    \n",
    "    def generate_batch(self, batch_size):  \n",
    "        #根据批的大小生成批数据的索引，如果shuffle则打乱数据\n",
    "        if self.shuffle:  #如果需要打乱数据\n",
    "            shuffled_arg = np.arange(self.length)  \n",
    "            #生成array([0,1,...,样本长度-1])\n",
    "            np.random.shuffle(shuffled_arg)  \n",
    "            #随机打乱shuffled_arg的顺序\n",
    "            self.inputs = self.inputs[shuffled_arg]  \n",
    "            #按照shuffled_arg来索引输入数据\n",
    "            self.mask = self.mask[shuffled_arg]   \n",
    "            #按照shuffled_arg来索引面罩数据\n",
    "            self.targets = self.targets[shuffled_arg]  \n",
    "            #按照shuffled_arg来索引预测目标数据\n",
    "        n_batch = int(self.length / batch_size)  #得到训练批数\n",
    "        if self.length % batch_size != 0: #批数需要取向上取整\n",
    "            n_batch += 1\n",
    "        #所有数据按照批进行拆分。eg:[0,..,99][100,..,199]...\n",
    "        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n",
    "        #最后一批有多少给多少。eg:[500,..506]\n",
    "        slices[-1] = slices[-1][:(self.length - batch_size * (n_batch - 1))]  \n",
    "        # 批量生成batch数据\n",
    "        return slices\n",
    "    \n",
    "    def get_slice(self, i):  \n",
    "            #根据索引i得到对应的数据，也就是第i个session\n",
    "            inputs, mask, targets = self.inputs[i], self.mask[i], self.targets[i]\n",
    "            #得到对应索引的输入，面罩，目标数据\n",
    "            \n",
    "            items, n_node, A, alias_inputs = [], [], [], []\n",
    "            \n",
    "            for u_input in inputs:\n",
    "                n_node.append(len(np.unique(u_input)))  \n",
    "                #n_node存储每个输入序列单独出现的点击动作类别的个数的列表\n",
    "            \n",
    "            max_n_node = np.max(n_node)   \n",
    "            #得到批最长唯一动作会话序列的长度\n",
    "            \n",
    "            for u_input in inputs:\n",
    "                node = np.unique(u_input)  \n",
    "                #该循环的会话的唯一动作序列\n",
    "                \n",
    "                items.append(node.tolist() + (max_n_node - len(node)) * [0])  \n",
    "                #单个点击动作序列的唯一类别并按照批最大类别补全0\n",
    "                \n",
    "                u_A = np.zeros((max_n_node, max_n_node)) \n",
    "                #存储行为矩阵的二维向量(方阵)，长度是最大唯一动作的数量\n",
    "                \n",
    "                #循环该序列的长度，就是循环session的所有节点\n",
    "                for i in np.arange(len(u_input) - 1):  \n",
    "                    \n",
    "                    #循环到i的下一个动作时“0”动作时退出循环，\n",
    "                    #因为0代表序列已经结束，后面都是补的动作0\n",
    "                    if u_input[i + 1] == 0:  \n",
    "                        break\n",
    "                \n",
    "                    u = np.where(node == u_input[i])[0][0] \n",
    "                    #该动作对应唯一动作集合的序号\n",
    "                    v = np.where(node == u_input[i + 1])[0][0] \n",
    "                    #下一个动作对应唯一动作集合的序号\n",
    "                    \n",
    "                    #前一个动作u_input[i]转移到后一个\n",
    "                    #动作u_input[i + 1]的次数变成1\n",
    "                    u_A[u][v] = 1  \n",
    "                    \n",
    "                u_sum_in = np.sum(u_A, 0) #矩阵列求和，最后变成一行\n",
    "                u_sum_in[np.where(u_sum_in == 0)] = 1\n",
    "                u_A_in = np.divide(u_A, u_sum_in)\n",
    "                u_sum_out = np.sum(u_A, 1) #矩阵行求和，最后变成一列\n",
    "                u_sum_out[np.where(u_sum_out == 0)] = 1\n",
    "                u_A_out = np.divide(u_A.transpose(), u_sum_out)\n",
    "                \n",
    "                #得到一个会话的连接矩阵\n",
    "                u_A = np.concatenate([u_A_in, u_A_out]).transpose()  \n",
    "                \n",
    "                \n",
    "                #存储该批数据图矩阵的列表，\n",
    "                #u_A方阵的长度相同——为该批最长唯一动作会话序列的长度\n",
    "                A.append(u_A)  \n",
    "                \n",
    "                #动作序列对应唯一动作集合的位置角标\n",
    "                alias_inputs.append([np.where(node == i)[0][0] for i in u_input]) \n",
    "\n",
    "            #返回：动作序列对应唯一动作集合的位置角标，\n",
    "            #该批数据图矩阵的列表，单个点击动作序列的\n",
    "            #唯一类别并按照批最大类别补全0列表，面罩，目标数据\n",
    "            return alias_inputs, A, items, mask, targets   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data() 对原始数据做简单的处理，方便对模型的喂养\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Data(train_data,method=opt.method,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [15, 16, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [15, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [15,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [19,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [21,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [22, 22, 23, 23, 23, 22, 23,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [22, 22, 23, 23, 23, 22,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [22, 22, 23, 23, 23,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.inputs[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,   2,   5, ..., 287, 287, 287])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = Data(test_data,method=opt.method,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.创建模型 SR-GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, Parameter\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-625f4ba47df0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mGNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m#输入仅需确定隐状态数和步数\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m  \u001b[1;31m#gnn前向传播的步数 default=1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Module' is not defined"
     ]
    }
   ],
   "source": [
    "class GNN(Module):\n",
    "#     该函数主要是为了进行带有门控的GNN部分\n",
    "    def __init__(self, hidden_size, step=1):  #输入仅需确定隐状态数和步数\n",
    "        super(GNN, self).__init__()\n",
    "        self.step = step  #gnn前向传播的步数 default=1\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = hidden_size * 2\n",
    "        self.gate_size = 3 * hidden_size\n",
    "        #有关Parameter函数的解释：首先可以把这个函数理解为类型转换函数，将一个不可训练的类型Tensor转换成可以训练的类型parameter\n",
    "        #并将这个parameter绑定到这个module里面(net.parameter()中就有这个绑定的parameter，所以在参数优化的时候可以进行优化的)，\n",
    "        #所以经过类型转换这个self.XX变成了模型的一部分，成为了模型中根据训练可以改动的参数了。\n",
    "        #使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。——————https://www.jianshu.com/p/d8b77cc02410\n",
    "        self.w_ih = Parameter(torch.Tensor(self.gate_size, self.input_size))\n",
    "        self.w_hh = Parameter(torch.Tensor(self.gate_size, self.hidden_size))\n",
    "        self.b_ih = Parameter(torch.Tensor(self.gate_size))\n",
    "        self.b_hh = Parameter(torch.Tensor(self.gate_size))\n",
    "        self.b_iah = Parameter(torch.Tensor(self.hidden_size))\n",
    "        self.b_oah = Parameter(torch.Tensor(self.hidden_size))\n",
    "        #有关nn.Linear的解释：torch.nn.Linear(in_features, out_features, bias=True)，对输入数据做线性变换：y=Ax+b\n",
    "        #形状：输入: (N,in_features)  输出： (N,out_features)\n",
    "#         这是公式（1）中需要进行线性变换\n",
    "        self.linear_edge_in = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.linear_edge_out = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.linear_edge_f = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "\n",
    "    def GNNCell(self, A, hidden):\n",
    "        #A-->实际上是该批数据图矩阵的列表  eg:(100,5?,10?(即5?X2))\n",
    "        #hidden--> eg(100-batch_size,5?,100-embeding_size) \n",
    "        #后面所有的5?代表这个维的长度是该批唯一最大类别长度(类别数目不足该长度的会话补零)，根据不同批会变化\n",
    "        #有关matmul的解释：矩阵相乘，多维会广播相乘  \n",
    "        \n",
    "        #这里是文中的公式1 ，对  input_in   input_out  分别做线性变换\n",
    "        input_in = torch.matmul(A[:, :, :A.shape[1]], self.linear_edge_in(hidden)) + self.b_iah   #input_in-->(100,5?,100)\n",
    "        input_out = torch.matmul(A[:, :, A.shape[1]: 2 * A.shape[1]], self.linear_edge_out(hidden)) + self.b_oah  #input_out-->(100,5?,100)\n",
    "        \n",
    "        #在第2个轴将tensor连接起来\n",
    "        #这里就是文中的asi\n",
    "        inputs = torch.cat([input_in, input_out], 2)  #inputs-->(100,5?,200)\n",
    "        \n",
    "        #关于functional.linear(input, weight, bias=None)的解释：y= xA^T + b 应用线性变换，返回Output: (N,∗,out_features)\n",
    "        #[*代表任意其他的东西]\n",
    "        gi = F.linear(inputs, self.w_ih, self.b_ih) #gi-->(100,5?,300)\n",
    "        gh = F.linear(hidden, self.w_hh, self.b_hh) #gh-->(100,5?,300)\n",
    "        #torch.chunk(tensor, chunks, dim=0)：将tensor拆分成指定数量的块，比如下面就是沿着第2个轴拆分成3块\n",
    "        i_r, i_i, i_n = gi.chunk(3, 2)  #三个都是(100,5?,100)\n",
    "        h_r, h_i, h_n = gh.chunk(3, 2)  #三个都是(100,5?,100)\n",
    "        resetgate = torch.sigmoid(i_r + h_r)   #resetgate-->(100,5?,100)      原文公式(3)\n",
    "        inputgate = torch.sigmoid(i_i + h_i)   #inputgate-->(100,5?,100)\n",
    "        newgate = torch.tanh(i_n + resetgate * h_n)  #newgate-->(100,5?,100)  原文公式(4)\n",
    "        hy = newgate + inputgate * (hidden - newgate)   #hy-->(100,5?,100)    原文公式(5)\n",
    "        return hy\n",
    "    \n",
    "    \n",
    "    def forward(self, A, hidden): \n",
    "        #A-->实际上是该批数据图矩阵的列表 eg:(100,5?,10?(即5?X2)) 5?代表这个维的长度是该批唯一最大类别长度(类别数目不足该长度的会话补零)，根据不同批会变化\n",
    "        #hidden--> eg:(100-batch_size,5?,100-embeding_size) 即数据图中节点类别对应低维嵌入的表示\n",
    "        \n",
    "        #按照step进行多次的GRU学习，最终得到节点向量\n",
    "        for i in range(self.step):  \n",
    "            hidden = self.GNNCell(A, hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionGraph(Module):\n",
    "    进行\n",
    "    def __init__(self, opt, n_node): #opt-->可控输入参数, n_node-->嵌入层图的节点数目\n",
    "        super(SessionGraph, self).__init__()\n",
    "        self.hidden_size = opt.hiddenSize  #opt.hiddenSize-->hidden state size\n",
    "        self.n_node = n_node\n",
    "        self.batch_size = opt.batchSize   #opt.batch_siza-->input batch size *default=100\n",
    "        self.nonhybrid = opt.nonhybrid   #opt.nonhybrid-->only use the global preference to predicts\n",
    "        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n",
    "        self.gnn = GNN(self.hidden_size, step=opt.step) #opt.step-->gnn propogation steps\n",
    "        self.linear_one = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.linear_two = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.linear_three = nn.Linear(self.hidden_size, 1, bias=False)\n",
    "        self.linear_transform = nn.Linear(self.hidden_size * 2, self.hidden_size, bias=True)\n",
    "        self.loss_function = nn.CrossEntropyLoss()  #交叉熵损失\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=opt.lr, weight_decay=opt.l2) #Adam优化算法\n",
    "        #StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1) 将每个参数组的学习率设置为每个step_size epoch\n",
    "        #由gamma衰减的初始lr。当last_epoch=-1时，将初始lr设置为lr。\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=opt.lr_dc_step, gamma=opt.lr_dc)\n",
    "        self.reset_parameters()   #初始化权重参数\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def compute_scores(self, hidden, mask):\n",
    "        #hidden-->(100,16?,100) 其中16?代表该样本所有数据最长会话的长度(不同数据集会不同)，单个样本其余部分补了0\n",
    "        #mask-->(100,16?) 有序列的位置是[1],没有动作序列的位置是[0]\n",
    "        #计算软注意力的机制，最终算出得分\n",
    "        \n",
    "        #这里是将所有的节点嵌入向量中最后一个 作为session的局部嵌入向量\n",
    "        ht = hidden[torch.arange(mask.shape[0]).long(), torch.sum(mask, 1) - 1]  # batch_size x latent_size(100,100) 这是最后一个动作对应的位置，即文章中说的局部偏好\n",
    "        \n",
    "        #这里对应公式6里面的 W1vn + W2vi + c\n",
    "        q1 = self.linear_one(ht).view(ht.shape[0], 1, ht.shape[1])  # batch_size x 1 x latent_size(100,1,100) 局部偏好线性变换后改成能计算的维度\n",
    "        q2 = self.linear_two(hidden)  # batch_size x seq_length x latent_size (100,16?,100) 即全局偏好\n",
    "        alpha = self.linear_three(torch.sigmoid(q1 + q2))  #(100,16,1)\n",
    "        \n",
    "        #这里的a就是公式6里的Sg 代表session的全局嵌入向量\n",
    "        a = torch.sum(alpha * hidden * mask.view(mask.shape[0], -1, 1).float(), 1) #(100,100)  原文中公式(6)\n",
    "        if not self.nonhybrid:\n",
    "            a = self.linear_transform(torch.cat([a, ht], 1))  #原文中公式(7)\n",
    "\n",
    "        #b就是候选需要打分的item\n",
    "        b = self.embedding.weight[1:]  # n_nodes x latent_size  (309,100)\n",
    "        \n",
    "        #通过乘积得到候选item的得分\n",
    "        scores = torch.matmul(a, b.transpose(1, 0))   #原文中公式(8)\n",
    "        return scores  #(100,309)\n",
    "\n",
    "    def forward(self, inputs, A):  \n",
    "        #inputs-->单个点击动作序列的唯一类别并按照批最大唯一类别长度补全0列表(即图矩阵的元素的类别标签列表)  A-->实际上是该批数据图矩阵的列表\n",
    "#        print(inputs.size())  #测试打印下输入的维度  （100-batch_size,5?） 5?代表这个维的长度是该批唯一最大类别长度(类别数目不足该长度的会话补0)，根据不同批会变化\n",
    "        hidden = self.embedding(inputs) #返回的hidden的shape -->（100-batch_size,5?,100-embeding_size）\n",
    "        #这里就是通过gru的GNN学习的节点向量\n",
    "        hidden = self.gnn(A, hidden)\n",
    "        return hidden  #(100,5?,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用cpu  还是Gpu  进行训练\n",
    "def trans_to_cuda(variable):\n",
    "    if torch.cuda.is_available():\n",
    "        return variable.cuda()\n",
    "    else:\n",
    "        return variable\n",
    "\n",
    "\n",
    "def trans_to_cpu(variable):\n",
    "    if torch.cuda.is_available():\n",
    "        return variable.cpu()\n",
    "    else:\n",
    "        return variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>定义模型在进行前传的时候的预处理，以及结合GNN以及计算得分两部分</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, i, data):  #传入模型model(SessionGraph), 数据批的索引i, 训练的数据data(Data)\n",
    "    #返回：动作序列对应唯一动作集合的位置角标，该批数据图矩阵的列表，单个点击动作序列的唯一类别并按照批最大类别补全0列表，面罩，目标数据\n",
    "    alias_inputs, A, items, mask, targets = data.get_slice(i)  \n",
    "    alias_inputs = trans_to_cuda(torch.Tensor(alias_inputs).long())  #(100,16?)\n",
    "    test_alias_inputs = alias_inputs.numpy()  #测试查看alias_inputs的内容\n",
    "    strange = torch.arange(len(alias_inputs)).long() #0到99\n",
    "    items = trans_to_cuda(torch.Tensor(items).long())\n",
    "    A = trans_to_cuda(torch.Tensor(A).float())\n",
    "    mask = trans_to_cuda(torch.Tensor(mask).long())\n",
    "    hidden = model(items, A)  #这里调用了SessionGraph的forward函数,返回维度数目(100,5?,100)\n",
    "    get = lambda i: hidden[i][alias_inputs[i]]   #选择第这一批第i个样本对应类别序列的函数\n",
    "    test_get = get(0)  # (16?,100)\n",
    "    seq_hidden = torch.stack([get(i) for i in torch.arange(len(alias_inputs)).long()])  #(100,16?,100)\n",
    "    return targets, model.compute_scores(seq_hidden, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #模型构建就靠这句话\n",
    "model = trans_to_cuda(SessionGraph(opt, n_node)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.构造训练\n",
    "\n",
    "主要是进行模型的传参，计算损失，反传，优化等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, train_data, test_data): #传入模型SessionGraph，训练数据和测试数据Data\n",
    "    model.scheduler.step()  #调度设置优化器的参数\n",
    "    print('start training: ', datetime.datetime.now())\n",
    "    model.train()  # 指定模型为训练模式，计算梯度\n",
    "    total_loss = 0.0\n",
    "    slices = train_data.generate_batch(model.batch_size)\n",
    "    for i, j in zip(slices, np.arange(len(slices))):   #根据批的索引数据进行数据提取训练:i-->批索引, j-->第几批\n",
    "        model.optimizer.zero_grad()  #前一步的损失清零\n",
    "        targets, scores = forward(model, i, train_data) #\n",
    "        targets = trans_to_cuda(torch.Tensor(targets).long())\n",
    "        loss = model.loss_function(scores, targets - 1)\n",
    "        loss.backward() # 反向传播\n",
    "        model.optimizer.step()  # 优化\n",
    "        total_loss += loss\n",
    "        if j % int(len(slices) / 5 + 1) == 0:\n",
    "            print('[%d/%d] Loss: %.4f' % (j, len(slices), loss.item()))\n",
    "    print('\\tLoss:\\t%.3f' % total_loss)\n",
    "\n",
    "    print('start predicting: ', datetime.datetime.now())\n",
    "    model.eval()  # 指定模型为计算模式\n",
    "    hit, mrr = [], []\n",
    "    slices = test_data.generate_batch(model.batch_size)\n",
    "    for i in slices:\n",
    "        targets, scores = forward(model, i, test_data)\n",
    "        sub_scores = scores.topk(20)[1]\n",
    "        sub_scores = trans_to_cpu(sub_scores).detach().numpy()\n",
    "        for score, target, mask in zip(sub_scores, targets, test_data.mask):\n",
    "            hit.append(np.isin(target - 1, score))\n",
    "            if len(np.where(score == target - 1)[0]) == 0:\n",
    "                mrr.append(0)\n",
    "            else:\n",
    "                mrr.append(1 / (np.where(score == target - 1)[0][0] + 1))\n",
    "    hit = np.mean(hit) * 100\n",
    "    mrr = np.mean(mrr) * 100\n",
    "    return hit, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    start = time.time()\n",
    "    best_result = [0, 0]\n",
    "    best_epoch = [0, 0]\n",
    "    bad_counter = 0\n",
    "    for epoch in range(opt.epoch):\n",
    "        print('-------------------------------------------------------')\n",
    "        print('epoch: ', epoch)\n",
    "        hit, mrr = train_test(model, train_data, test_data)  #模型训练就靠这句话\n",
    "        flag = 0\n",
    "        if hit >= best_result[0]:\n",
    "            best_result[0] = hit\n",
    "            best_epoch[0] = epoch\n",
    "            flag = 1\n",
    "        if mrr >= best_result[1]:\n",
    "            best_result[1] = mrr\n",
    "            best_epoch[1] = epoch\n",
    "            flag = 1\n",
    "        print('Best Result:')\n",
    "        print('\\tRecall@20:\\t%.4f\\tMMR@20:\\t%.4f\\tEpoch:\\t%d,\\t%d'% (best_result[0], best_result[1], best_epoch[0], best_epoch[1]))\n",
    "        bad_counter += 1 - flag\n",
    "        if bad_counter >= opt.patience:\n",
    "            break\n",
    "    print('-------------------------------------------------------')\n",
    "    end = time.time()\n",
    "    print(\"Run time: %f s\" % (end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
